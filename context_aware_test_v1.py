# -*- coding: utf-8 -*-
"""Context_Aware_Test_V1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GaDWE4DkJ6sKdig7vvmL76vlGNiJlrMF
"""

# keyboard based, not machine learning. new version with ML is done

# Import required libraries
import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer

# Load the model and tokenizer
model_name = "distilgpt2"
model = GPT2LMHeadModel.from_pretrained(model_name)
tokenizer = GPT2Tokenizer.from_pretrained(model_name)

# Define keywords associated with each context
context_keywords = {
    "work": ["meeting", "project", "deadline", "email", "client"],
    "social": ["friends", "party", "movie", "hangout", "chat"],
    "general": []  # Default context if no keywords match
}

# Function to detect context based on keywords
def detect_context(input_text):
    for context, keywords in context_keywords.items():
        if any(keyword in input_text.lower() for keyword in keywords):
            return context
    return "general"  # Default context if no keywords match

# Function to generate predictive text with context-aware adjustments
def generate_predictions(input_text, max_length=20, num_return_sequences=3, temperature=0.7, top_p=0.9):
    # Detect context
    context = detect_context(input_text)
    print(f"Detected context: {context}")

    # Encode the input text to tensor format with attention mask
    inputs = tokenizer.encode(input_text, return_tensors="pt")
    attention_mask = torch.ones(inputs.shape, dtype=torch.long)

    # Adjust max_length or num_return_sequences based on context
    if context == "work":
        max_length = 15
    elif context == "social":
        max_length = 25

    # Generate text suggestions with beam search and sampling parameters
    outputs = model.generate(
        inputs,
        attention_mask=attention_mask,
        max_length=max_length,
        num_return_sequences=num_return_sequences,
        no_repeat_ngram_size=2,          # Avoid repetition
        early_stopping=True,             # Stop generation once criteria are met
        num_beams=3,                     # Enable beam search
        pad_token_id=tokenizer.eos_token_id,  # Set pad_token_id to eos_token_id
        temperature=temperature,         # Added temperature for randomness
        top_p=top_p                      # Added top_p for nucleus sampling
    )

    # Decode predictions
    predictions = [tokenizer.decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=True) for output in outputs]
    return predictions

# Test inputs with different contexts
test_inputs = [
    "I need to prepare for a client meeting",  # Work context
    "Let's go watch a movie with friends",     # Social context
    "I need help with this",                   # General context
]

# Generate and display predictions for each test input
for input_text in test_inputs:
    print(f"\nInput: {input_text}")
    predictions = generate_predictions(input_text)
    for i, prediction in enumerate(predictions, 1):
        print(f"Prediction {i}: {prediction}")